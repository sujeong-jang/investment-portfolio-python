# -*- coding: utf-8 -*-
"""투자성향 군집 및 분류.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Mcwvmi7ZCQuUXOp8PopdH2_tDjL7PcWZ

## Importing Libraries
"""

import numpy as np
import pandas as pd
from pandas import plotting
import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use('fivethirtyeight')

from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_samples, silhouette_score, confusion_matrix, accuracy_score
from sklearn.metrics import accuracy_score,confusion_matrix,precision_score, recall_score, f1_score, roc_auc_score
from sklearn.model_selection import train_test_split, KFold

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC

import warnings
warnings.filterwarnings('ignore')

"""## Loading Data"""

train = pd.read_csv('dataset/train.csv')
test = pd.read_csv('dataset/test.csv')
train.shape, test.shape

train.head()

"""## Preprocessing Data"""

train["score"] = train.sum(axis=1)
test["score"] = test.sum(axis=1)

"""## Clustering Analysis"""

def get_silhouette_score(df, model):
  new_df = df.copy()

  model.fit(new_df)
  new_df['target'] = model.labels_

  pca = PCA(n_components=2)
  pca_transformed = pca.fit_transform(new_df)

  new_df['pca_x'] = pca_transformed[:, 0]
  new_df['pca_y'] = pca_transformed[:, 1]

  plt.scatter(x=new_df.loc[:, 'pca_x'], y=new_df.loc[:, 'pca_y'], c=new_df['target'])
  plt.figure()

  average_score = silhouette_score(new_df, new_df['target'])
  print(f'Silhouette Analysis Score: {average_score}')

  new_df.drop(['pca_x', 'pca_y'], axis=1, inplace=True)

  return new_df

from sklearn.cluster import MiniBatchKMeans

mini = MiniBatchKMeans(n_clusters = 5, random_state = 0)
cluster_df = get_silhouette_score(train, mini)

"""## Classification"""

cluster_df.head()

y = cluster_df["target"]
X = cluster_df.drop("target", axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

from sklearn.model_selection import GridSearchCV

params = {
    'n_estimators': [100],
    'max_depth': [6, 8, 10],
    'min_samples_leaf': [8, 12, 18],
    'min_samples_split': [8, 16, 20]
}

# RandomForestClassifier 객체 생성 후 GridSearchCV 수행
rf_clf = RandomForestClassifier(random_state=0)
grid_cv = GridSearchCV(rf_clf, param_grid=params, cv=5)
grid_cv.fit(X_train , y_train)

print(f'최적의 하이퍼 파라미터: {grid_cv.best_params_}')
print(f'최고 예측 정확도: {grid_cv.best_score_:.4f}')

rf_clf = RandomForestClassifier(n_estimators=100, max_depth=8, min_samples_leaf=8, min_samples_split=8, random_state=0)
rf_clf.fit(X_train, y_train)
X_test['target'] = rf_clf.predict(X_test)
print('The accuracy of the Random Forests is', accuracy_score(X_test['target'], y_test))

def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix(y_test, pred)
    accuracy = accuracy_score(y_test , pred)
    precision = precision_score(y_test , pred, average='micro')
    recall = recall_score(y_test , pred, average='macro')
    f1 = f1_score(y_test,pred,average='macro')
    # ROC-AUC 추가 
    roc_auc = roc_auc_score(y_test, pred_proba, multi_class="ovr")
    print('오차 행렬')
    print(confusion)
    # ROC-AUC print 추가
    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}, F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))

#예측 성능 평가
proba = rf_clf.predict_proba(X_test.drop(['target'], axis=1))
get_clf_eval(y_test,X_test['target'],proba)

"""### Feature importance"""

ftr_importances_values = rf_clf.feature_importances_
ftr_importances = pd.Series(ftr_importances_values, index=X_train.columns)

ftr_top = ftr_importances.sort_values(ascending=False)
plt.figure(figsize=(7, 5))
plt.title('Feature importance')
sns.barplot(x=ftr_top, y = ftr_top.index)
plt.show()

def desc(X, col):
    targets = []
    target_type = X.groupby('target')[col].mean().sort_values().index

    for i in target_type:
        target = X[X['target']==i][col].describe()
        targets.append(target)

    target_df = pd.DataFrame(targets).T
    target_df.columns = target_type
    return target_df

test['target'] = rf_clf.predict(test)
test.head()

desc(test, 'score')

desc(test, 'age')

desc(test, 'risk')

desc(test, 'income')

desc(test, 'knowledge')

desc(test, 'exp')

desc(test, 's1')

desc(test, 's2')

"""<pre>
안정형: 1
안정추구형: 0
위험중립형: 4
적극투자형: 2
공격투자형: 3
</pre>

### Saving model
"""

import joblib

joblib.dump(rf_clf, './models/rf_model.pkl')

test.head()